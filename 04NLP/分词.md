# 理论
机器学习：HMM
深度学习：LSTM+CRF, BERT+CRF

CRF模型

维特比算法

工具：jionlp


# Jieba
jieba没有实现CRF, CRF比HMM好
提供了深度学习

## 默认参数
返回generator
``` python
def cut(self, sentence, cut_all=False, HMM=True, use_paddle=False)
```
返回list
```python
jieba.lcut()
```

调整词频
```python
jieba.suggest_freq('占比', tune=False) # tune = False, 打印词频，不调整词频
jieba.suggest_freq(('中', '将'), True) #修改词频 强制“中将” 分开
jieba.suggest_freq('占比', True) #强制让“占比”作为一次词
```

## 词性标注

```python
import jieba.posseg
```


## 并行分词

```python
jieba.enable_parallel() #只能在Linux
```

加载字典
```python
jieba.load_userdict("./jieba.dict")

# jieba.dict
# 饿了么 100
```



## 修改词频

```python
jieba.suggest_freq(('中', '将'), tune=True)
```

## 词性
``` python
words = posseg.cut("外卖送餐公司中饿了么是你值得信赖的选择，如果放到post中将出错")  
print(list(words))

# [pair('外卖', 'v'), pair('送餐', 'v'), pair('公司', 'n'), pair('中', 'f'), pair('饿了么', 'x'), pair('是', 'v'), pair('你', 'r'), pair('值得', 'v'), pair('信赖', 'n'), pair('的', 'uj'), pair('选择', 'v'), pair('，', 'x'), pair('如果', 'c'), pair('放到', 'v'), pair('post', 'eng'), pair('中将', 'n'), pair('出错', 'v')]

```

# HanLP
用BERT分词
提供CRF

- BasicToknerizer: 机器切分，维特比算法
- SpeedToknerizer: 词典，速度快，效果差
- IndexToknerizer: 搜索引擎分词器，类似于jie.cut_for_search
- CRFLexicalAnalyzer: CRF分词，同时返回分词，词性和命名实体
