# 语言模型
计算一句话出现的概率， P(武松打死了老虎) > P(老虎打死了武松)

## N-gram 概率统计的语言模型
使用朴素贝叶斯和马尔可夫假设, 统计语言模型

## NNLM 神经语言模型
Neural Network Language Modeling 前向滑动窗口，非两端 计算量大

# 词向量
one-hot


## word2vec
不能算神经网络，只是浅层的神经网络

与NNLM 相比，word2vec 的主要目的是**生成词向量而不是语言模型**，在CBOW中，投射层将词向量直接相加而不是拼接起来，并**舍弃了隐层**，这些牺牲都是为了减少计算量

CBOW: 两边预测中间
Skip-Gram: 中间预测2端，能够覆盖的词更多
CBOW: 通过上下文词预测中心词 Skip-Gram: 通过中心词预测上下文词

优化技术： Hierarchical Softmax：为了解决输出层的计算量，使用Hierarchical Softmax, 层次化的哈夫曼树 负采样：

缺点：不能处理近义词，比如苹果和苹果手机

## fasttext
fastText 模型架构和 Word2Vec中的CBOW 模型很类似, 作者都是Facebook的科学家Tomas Mikolo。不同之处在于，fastText学习目标是人工标注的分类结果， fastText训练词向量时会考虑subword.

模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用；

模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容；

## GloVe
优化word2vec只能在固定窗口分析

word2vec是静态 bert: 动态词向量，处理近意词

# seq2seq
RNN使用GRU


资料
https://blog.csdn.net/v_JULY_v/article/details/102708459

基于矩阵分解的主题模型(矩阵分解)，基于神经网络的word2vec？
