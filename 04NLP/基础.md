# 语言模型
计算一句话出现的概率， P(武松打死了老虎) > P(老虎打死了武松)

## N-gram
使用朴素贝叶斯和马尔可夫假设, 统计语言模型

## NNLM
神经语言模型
Neural Network Language Modeling 前向滑动窗口，非两端 计算量大

# 词向量
one-hot


## word2vec
不能算神经网络，只是浅层的神经网络

word2vec优化了NNLM, 去除了隐层

CBOW: 两边预测中间
Skip-Gram: 中间预测2端，能够覆盖的词更多
CBOW: 通过上下文词预测中心词 Skip-Gram: 通过中心词预测上下文词

优化技术： Hierarchical Softmax：为了解决输出层的计算量，使用Hierarchical Softmax, 层次化的哈夫曼树 负采样：

缺点：不能处理近义词，比如苹果和苹果手机

## fasttext
fastText 模型架构和 Word2Vec 中的 CBOW 模型很类似, 作者都是Facebook的科学家Tomas Mikolo。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词.

模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用；

模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容；

## GloVe
优化word2vec只能在固定窗口分析

word2vec是静态 bert: 动态词向量，处理近意词


资料
https://blog.csdn.net/v_JULY_v/article/details/102708459

基于矩阵分解的主题模型(矩阵分解)，基于神经网络的word2vec？
